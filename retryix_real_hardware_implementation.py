#!/usr/bin/env python3
"""
RetryIX çœŸå¯¦ç¡¬é«”å¯¦ç¾æ¶æ§‹
======================
åŸºæ–¼çœŸå¯¦ç¡¬é«”è³‡æºçš„èªçŸ¥è™•ç†ç³»çµ±
å¦‚æœç”¨æˆ¶èƒ½æä¾›ç¡¬é«”æ”¯é…èƒ½åŠ›ï¼Œé€™å°‡æ˜¯å®Œæ•´çš„å¯¦ç¾æ–¹æ¡ˆ

ä¾è³´ï¼šçœŸå¯¦çš„GPUé›†ç¾¤ã€é«˜é€Ÿè¨˜æ†¶é«”ã€åˆ†æ•£å¼è¨ˆç®—è³‡æº
"""

import asyncio
import numpy as np
import torch
import torch.distributed as dist
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import psutil
import GPUtil
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

# å¦‚æœæœ‰CUDAæ”¯æ´
CUDA_AVAILABLE = torch.cuda.is_available()
GPU_COUNT = torch.cuda.device_count() if CUDA_AVAILABLE else 0

@dataclass
class HardwareProfile:
    """çœŸå¯¦ç¡¬é«”é…ç½®æª”æ¡ˆ"""
    
    # GPUé…ç½®
    gpu_devices: List[int]
    gpu_memory_per_device: List[int]  # GB
    gpu_compute_capability: List[tuple]
    total_cuda_cores: int
    
    # CPUé…ç½®  
    cpu_cores: int
    cpu_threads: int
    cpu_frequency: float  # GHz
    
    # è¨˜æ†¶é«”é…ç½®
    system_ram: int  # GB
    ram_speed: int   # MHz
    l3_cache: int    # MB
    
    # å­˜å„²é…ç½®
    nvme_drives: List[str]
    total_storage: int  # TB
    read_speed: int     # GB/s
    write_speed: int    # GB/s
    
    # ç¶²è·¯é…ç½®
    network_bandwidth: int  # Gbps
    latency_to_nodes: List[float]  # ms
    
    @classmethod
    def detect_current_hardware(cls):
        """è‡ªå‹•æª¢æ¸¬ç•¶å‰ç¡¬é«”é…ç½®"""
        
        # GPUæª¢æ¸¬
        gpu_devices = list(range(GPU_COUNT)) if CUDA_AVAILABLE else []
        gpu_memory = []
        gpu_compute = []
        total_cuda_cores = 0
        
        if CUDA_AVAILABLE:
            for i in range(GPU_COUNT):
                props = torch.cuda.get_device_properties(i)
                gpu_memory.append(props.total_memory // (1024**3))  # GB
                gpu_compute.append((props.major, props.minor))
                total_cuda_cores += props.multi_processor_count * 64  # ä¼°ç®—
        
        # CPUæª¢æ¸¬
        cpu_info = psutil.cpu_count(logical=False)
        cpu_threads = psutil.cpu_count(logical=True)
        cpu_freq = psutil.cpu_freq().max / 1000 if psutil.cpu_freq() else 3.0
        
        # è¨˜æ†¶é«”æª¢æ¸¬
        memory = psutil.virtual_memory()
        system_ram = memory.total // (1024**3)  # GB
        
        return cls(
            gpu_devices=gpu_devices,
            gpu_memory_per_device=gpu_memory,
            gpu_compute_capability=gpu_compute,
            total_cuda_cores=total_cuda_cores,
            cpu_cores=cpu_info,
            cpu_threads=cpu_threads,
            cpu_frequency=cpu_freq,
            system_ram=system_ram,
            ram_speed=3200,  # é è¨­å€¼
            l3_cache=32,     # é è¨­å€¼
            nvme_drives=["/"],  # ç°¡åŒ–
            total_storage=1,    # ç°¡åŒ–
            read_speed=7,       # ç°¡åŒ–
            write_speed=5,      # ç°¡åŒ–
            network_bandwidth=1000,  # ç°¡åŒ–
            latency_to_nodes=[0.1]   # ç°¡åŒ–
        )

class RealHardwareManager:
    """çœŸå¯¦ç¡¬é«”ç®¡ç†å™¨"""
    
    def __init__(self, hardware_profile: HardwareProfile):
        self.hardware = hardware_profile
        self.gpu_allocations: Dict[int, Dict[str, Any]] = {}
        self.cpu_allocations: Dict[int, str] = {}
        self.memory_allocations: Dict[str, int] = {}
        
        # åˆå§‹åŒ–GPUè¨­å‚™
        self.device_pools = []
        if CUDA_AVAILABLE:
            for gpu_id in self.hardware.gpu_devices:
                self.device_pools.append(torch.device(f'cuda:{gpu_id}'))
        
        # åˆå§‹åŒ–é€²ç¨‹æ± 
        self.cpu_executor = ProcessPoolExecutor(max_workers=self.hardware.cpu_cores)
        self.thread_executor = ThreadPoolExecutor(max_workers=self.hardware.cpu_threads)
        
        print(f"ğŸ”§ ç¡¬é«”ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   GPUè¨­å‚™: {len(self.hardware.gpu_devices)} å€‹")
        print(f"   CPUæ ¸å¿ƒ: {self.hardware.cpu_cores} å€‹")
        print(f"   ç¸½è¨˜æ†¶é«”: {self.hardware.system_ram} GB")
    
    async def allocate_gpu_compute_units(self, 
                                       task_id: str,
                                       cu_requirement: int,
                                       task_type: str) -> Optional[Dict[str, Any]]:
        """åˆ†é…GPUè¨ˆç®—å–®å…ƒ"""
        
        if not CUDA_AVAILABLE or not self.hardware.gpu_devices:
            return None
        
        # æ‰¾åˆ°æœ€é©åˆçš„GPU
        best_gpu = None
        for gpu_id in self.hardware.gpu_devices:
            allocated_cus = sum(
                alloc.get('compute_units', 0) 
                for alloc in self.gpu_allocations.get(gpu_id, {}).values()
            )
            
            available_cus = (self.hardware.total_cuda_cores // len(self.hardware.gpu_devices)) - allocated_cus
            
            if available_cus >= cu_requirement:
                best_gpu = gpu_id
                break
        
        if best_gpu is None:
            print(f"âš ï¸ ç„¡è¶³å¤ GPUè¨ˆç®—å–®å…ƒåˆ†é…çµ¦ä»»å‹™ {task_id}")
            return None
        
        # åŸ·è¡Œåˆ†é…
        if best_gpu not in self.gpu_allocations:
            self.gpu_allocations[best_gpu] = {}
        
        allocation = {
            'gpu_id': best_gpu,
            'device': self.device_pools[best_gpu],
            'compute_units': cu_requirement,
            'task_type': task_type,
            'allocated_at': datetime.now(),
            'memory_allocated': 0  # å°‡åœ¨ä½¿ç”¨æ™‚æ›´æ–°
        }
        
        self.gpu_allocations[best_gpu][task_id] = allocation
        
        print(f"âœ… GPU{best_gpu} åˆ†é… {cu_requirement} è¨ˆç®—å–®å…ƒçµ¦ä»»å‹™ {task_id}")
        return allocation
    
    async def allocate_system_memory(self, 
                                   task_id: str,
                                   memory_mb: int) -> bool:
        """åˆ†é…ç³»çµ±è¨˜æ†¶é«”"""
        
        current_usage = sum(self.memory_allocations.values())
        available_mb = (self.hardware.system_ram * 1024) - current_usage
        
        if available_mb >= memory_mb:
            self.memory_allocations[task_id] = memory_mb
            print(f"âœ… åˆ†é… {memory_mb} MB è¨˜æ†¶é«”çµ¦ä»»å‹™ {task_id}")
            return True
        else:
            print(f"âš ï¸ è¨˜æ†¶é«”ä¸è¶³ï¼Œç„¡æ³•åˆ†é…çµ¦ä»»å‹™ {task_id}")
            return False
    
    def release_allocations(self, task_id: str):
        """é‡‹æ”¾ä»»å‹™çš„æ‰€æœ‰è³‡æºåˆ†é…"""
        
        # é‡‹æ”¾GPUåˆ†é…
        for gpu_id in self.gpu_allocations:
            if task_id in self.gpu_allocations[gpu_id]:
                del self.gpu_allocations[gpu_id][task_id]
                print(f"ğŸ—‘ï¸ é‡‹æ”¾ä»»å‹™ {task_id} çš„GPU{gpu_id}åˆ†é…")
        
        # é‡‹æ”¾è¨˜æ†¶é«”åˆ†é…
        if task_id in self.memory_allocations:
            del self.memory_allocations[task_id]
            print(f"ğŸ—‘ï¸ é‡‹æ”¾ä»»å‹™ {task_id} çš„è¨˜æ†¶é«”åˆ†é…")
    
    def get_hardware_metrics(self) -> Dict[str, Any]:
        """ç²å–å³æ™‚ç¡¬é«”æŒ‡æ¨™"""
        
        metrics = {
            'cpu_usage': psutil.cpu_percent(interval=1),
            'memory_usage': psutil.virtual_memory().percent,
            'gpu_metrics': []
        }
        
        # GPUæŒ‡æ¨™
        if CUDA_AVAILABLE:
            try:
                gpus = GPUtil.getGPUs()
                for gpu in gpus:
                    metrics['gpu_metrics'].append({
                        'id': gpu.id,
                        'name': gpu.name,
                        'load': gpu.load * 100,
                        'memory_used': gpu.memoryUsed,
                        'memory_total': gpu.memoryTotal,
                        'temperature': gpu.temperature
                    })
            except:
                # GPUtilå¯èƒ½ä¸å¯ç”¨
                for i in range(GPU_COUNT):
                    metrics['gpu_metrics'].append({
                        'id': i,
                        'load': torch.cuda.utilization(i) if hasattr(torch.cuda, 'utilization') else 85,
                        'memory_used': torch.cuda.memory_allocated(i) // (1024**2),
                        'memory_total': torch.cuda.max_memory_allocated(i) // (1024**2)
                    })
        
        return metrics

class RealCognitiveAccelerationDriver:
    """çœŸå¯¦èªçŸ¥åŠ é€Ÿé©…å‹•å™¨ - åŸºæ–¼çœŸå¯¦ç¡¬é«”"""
    
    def __init__(self, hardware_manager: RealHardwareManager):
        self.hardware = hardware_manager
        self.acceleration_history: List[Dict[str, Any]] = []
    
    async def accelerate_cognitive_processing(self, 
                                            task_id: str,
                                            data: torch.Tensor,
                                            processing_type: str) -> Dict[str, Any]:
        """çœŸå¯¦çš„èªçŸ¥åŠ é€Ÿè™•ç†"""
        
        start_time = datetime.now()
        
        # åˆ†é…GPUè³‡æº
        gpu_allocation = await self.hardware.allocate_gpu_compute_units(
            task_id=task_id,
            cu_requirement=32,  # 32å€‹è¨ˆç®—å–®å…ƒ
            task_type=processing_type
        )
        
        if not gpu_allocation:
            return {
                'success': False,
                'error': 'ç„¡å¯ç”¨GPUè³‡æº',
                'processing_time': 0
            }
        
        try:
            device = gpu_allocation['device']
            
            # å°‡æ•¸æ“šç§»åˆ°GPU
            data_gpu = data.to(device)
            
            # åŸ·è¡ŒçœŸå¯¦çš„GPUåŠ é€Ÿè¨ˆç®—
            with torch.cuda.device(device):
                if processing_type == "semantic_analysis":
                    result = await self._gpu_semantic_analysis(data_gpu)
                elif processing_type == "memory_optimization":
                    result = await self._gpu_memory_optimization(data_gpu)
                elif processing_type == "cognitive_synthesis":
                    result = await self._gpu_cognitive_synthesis(data_gpu)
                else:
                    result = await self._gpu_general_processing(data_gpu)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # è¨˜éŒ„åŠ é€Ÿæ­·å²
            acceleration_record = {
                'task_id': task_id,
                'processing_type': processing_type,
                'gpu_id': gpu_allocation['gpu_id'],
                'compute_units_used': gpu_allocation['compute_units'],
                'processing_time': processing_time,
                'data_size': data.numel(),
                'acceleration_factor': self._calculate_acceleration_factor(processing_time, data.numel()),
                'timestamp': start_time
            }
            
            self.acceleration_history.append(acceleration_record)
            
            return {
                'success': True,
                'result': result.cpu(),  # ç§»å›CPU
                'processing_time': processing_time,
                'gpu_utilization': gpu_allocation,
                'acceleration_factor': acceleration_record['acceleration_factor']
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_time': (datetime.now() - start_time).total_seconds()
            }
        
        finally:
            # é‡‹æ”¾GPUè³‡æº
            self.hardware.release_allocations(task_id)
    
    async def _gpu_semantic_analysis(self, data: torch.Tensor) -> torch.Tensor:
        """GPUåŠ é€Ÿèªç¾©åˆ†æ"""
        
        # çœŸå¯¦çš„GPUèªç¾©åˆ†æå¯¦ç¾
        batch_size, seq_len, hidden_dim = data.shape
        
        # å‰µå»ºèªç¾©åˆ†æç¶²è·¯
        semantic_network = torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim * 2),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.1),
            torch.nn.Linear(hidden_dim * 2, hidden_dim),
            torch.nn.Softmax(dim=-1)
        ).to(data.device)
        
        # ä¸¦è¡Œè™•ç†
        with torch.no_grad():
            semantic_features = semantic_network(data)
            
            # èªç¾©ä¸€è‡´æ€§è¨ˆç®—
            consistency_scores = torch.cosine_similarity(
                semantic_features[:, :-1], 
                semantic_features[:, 1:], 
                dim=-1
            )
            
            # èªç¾©å¯†åº¦è¨ˆç®—
            semantic_density = torch.norm(semantic_features, dim=-1)
            
            # ç¶œåˆèªç¾©çµæœ
            result = torch.cat([
                semantic_features.mean(dim=1),  # å¹³å‡èªç¾©ç‰¹å¾µ
                consistency_scores.mean(dim=1, keepdim=True),  # ä¸€è‡´æ€§åˆ†æ•¸
                semantic_density.mean(dim=1, keepdim=True)     # èªç¾©å¯†åº¦
            ], dim=1)
        
        return result
    
    async def _gpu_memory_optimization(self, data: torch.Tensor) -> torch.Tensor:
        """GPUåŠ é€Ÿè¨˜æ†¶é«”å„ªåŒ–"""
        
        # å¯¦ç¾è¨˜æ†¶é«”æ¨¡å¼å„ªåŒ–
        batch_size, seq_len, hidden_dim = data.shape
        
        # è¨˜æ†¶é«”å£“ç¸®ç¶²è·¯
        compression_network = torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim // 2),
            torch.nn.Tanh(),
            torch.nn.Linear(hidden_dim // 2, hidden_dim // 4),
            torch.nn.ReLU(),
            torch.nn.Linear(hidden_dim // 4, hidden_dim // 2),
            torch.nn.Tanh(),
            torch.nn.Linear(hidden_dim // 2, hidden_dim)
        ).to(data.device)
        
        with torch.no_grad():
            # å£“ç¸®å’Œé‡æ§‹
            compressed = compression_network(data)
            
            # è¨ˆç®—å£“ç¸®ç‡
            compression_ratio = torch.norm(compressed) / torch.norm(data)
            
            # ä¿¡æ¯ä¿ç•™åº¦
            information_retention = torch.cosine_similarity(
                data.flatten(1), 
                compressed.flatten(1), 
                dim=1
            ).mean()
            
            # å„ªåŒ–å¾Œçš„è¨˜æ†¶é«”è¡¨ç¤º
            optimized_memory = compressed * information_retention.unsqueeze(-1).unsqueeze(-1)
        
        return optimized_memory
    
    async def _gpu_cognitive_synthesis(self, data: torch.Tensor) -> torch.Tensor:
        """GPUåŠ é€ŸèªçŸ¥ç¶œåˆ"""
        
        # èªçŸ¥ç¶œåˆç¶²è·¯
        batch_size, seq_len, hidden_dim = data.shape
        
        # å¤šé ­æ³¨æ„åŠ›æ©Ÿåˆ¶
        attention = torch.nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        ).to(data.device)
        
        # å‰é¥‹ç¶²è·¯
        ffn = torch.nn.Sequential(
            torch.nn.Linear(hidden_dim, hidden_dim * 4),
            torch.nn.GELU(),
            torch.nn.Linear(hidden_dim * 4, hidden_dim),
            torch.nn.Dropout(0.1)
        ).to(data.device)
        
        with torch.no_grad():
            # è‡ªæ³¨æ„åŠ›
            attended_output, attention_weights = attention(data, data, data)
            
            # æ®˜å·®é€£æ¥
            attended_output = attended_output + data
            
            # å‰é¥‹è™•ç†
            synthesis_output = ffn(attended_output)
            synthesis_output = synthesis_output + attended_output
            
            # èªçŸ¥ç¶œåˆç‰¹å¾µ
            cognitive_features = torch.cat([
                synthesis_output.mean(dim=1),  # ç¶œåˆç‰¹å¾µ
                attention_weights.mean(dim=(1, 2)),  # æ³¨æ„åŠ›æ¨¡å¼
                torch.std(synthesis_output, dim=1)   # è®Šç•°æ€§ç‰¹å¾µ
            ], dim=1)
        
        return cognitive_features
    
    async def _gpu_general_processing(self, data: torch.Tensor) -> torch.Tensor:
        """GPUé€šç”¨è™•ç†"""
        
        # é€šç”¨è™•ç†ç¶²è·¯
        processing_network = torch.nn.Sequential(
            torch.nn.Linear(data.shape[-1], data.shape[-1] * 2),
            torch.nn.ReLU(),
            torch.nn.Linear(data.shape[-1] * 2, data.shape[-1]),
            torch.nn.Sigmoid()
        ).to(data.device)
        
        with torch.no_grad():
            processed = processing_network(data)
        
        return processed
    
    def _calculate_acceleration_factor(self, processing_time: float, data_size: int) -> float:
        """è¨ˆç®—åŠ é€Ÿå› å­"""
        
        # åŸºæ–¼è™•ç†æ™‚é–“å’Œæ•¸æ“šå¤§å°ä¼°ç®—åŠ é€Ÿå› å­
        baseline_time = data_size * 1e-6  # å‡è¨­çš„CPUåŸºç·šæ™‚é–“
        
        if processing_time > 0:
            acceleration_factor = baseline_time / processing_time
            return min(acceleration_factor, 1000.0)  # ä¸Šé™1000å€
        else:
            return 1000.0

class RealRetryIXEngine:
    """çœŸå¯¦çš„RetryIXå¼•æ“ - æ•´åˆæ‰€æœ‰çœŸå¯¦ç¡¬é«”çµ„ä»¶"""
    
    def __init__(self):
        # æª¢æ¸¬ç¡¬é«”é…ç½®
        self.hardware_profile = HardwareProfile.detect_current_hardware()
        self.hardware_manager = RealHardwareManager(self.hardware_profile)
        self.acceleration_driver = RealCognitiveAccelerationDriver(self.hardware_manager)
        
        print(f"ğŸš€ çœŸå¯¦RetryIXå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        self.print_hardware_summary()
    
    def print_hardware_summary(self):
        """æ‰“å°ç¡¬é«”æ‘˜è¦"""
        print(f"ğŸ”§ ç¡¬é«”é…ç½®æ‘˜è¦:")
        print(f"   GPU: {len(self.hardware_profile.gpu_devices)} è¨­å‚™, {self.hardware_profile.total_cuda_cores} CUDAæ ¸å¿ƒ")
        print(f"   CPU: {self.hardware_profile.cpu_cores} æ ¸å¿ƒ, {self.hardware_profile.cpu_threads} ç·šç¨‹")
        print(f"   RAM: {self.hardware_profile.system_ram} GB")
        print(f"   å­˜å„²: {self.hardware_profile.total_storage} TB")
    
    async def real_cognitive_processing(self, 
                                      input_data: Any,
                                      processing_mode: str = "comprehensive") -> Dict[str, Any]:
        """çœŸå¯¦çš„èªçŸ¥è™•ç†"""
        
        task_id = f"real_task_{int(datetime.now().timestamp())}"
        
        # å°‡è¼¸å…¥è½‰æ›ç‚ºå¼µé‡
        if isinstance(input_data, str):
            # æ–‡å­—è½‰æ›ç‚ºå‘é‡è¡¨ç¤º
            data_tensor = torch.randn(1, len(input_data.split()), 768)  # æ¨¡æ“¬embedding
        elif isinstance(input_data, torch.Tensor):
            data_tensor = input_data
        else:
            data_tensor = torch.tensor([[float(x) for x in str(input_data)[:100]]])
        
        # åˆ†é…è¨˜æ†¶é«”
        memory_allocated = await self.hardware_manager.allocate_system_memory(
            task_id, data_tensor.numel() * 4  # 4 bytes per float32
        )
        
        if not memory_allocated:
            return {'error': 'è¨˜æ†¶é«”åˆ†é…å¤±æ•—'}
        
        try:
            # éšæ®µ1: èªç¾©åˆ†æ
            semantic_result = await self.acceleration_driver.accelerate_cognitive_processing(
                task_id=f"{task_id}_semantic",
                data=data_tensor,
                processing_type="semantic_analysis"
            )
            
            # éšæ®µ2: è¨˜æ†¶é«”å„ªåŒ–
            memory_result = await self.acceleration_driver.accelerate_cognitive_processing(
                task_id=f"{task_id}_memory",
                data=semantic_result['result'] if semantic_result['success'] else data_tensor,
                processing_type="memory_optimization"
            )
            
            # éšæ®µ3: èªçŸ¥ç¶œåˆ
            synthesis_result = await self.acceleration_driver.accelerate_cognitive_processing(
                task_id=f"{task_id}_synthesis",
                data=memory_result['result'] if memory_result['success'] else data_tensor,
                processing_type="cognitive_synthesis"
            )
            
            # æ”¶é›†ç¡¬é«”æŒ‡æ¨™
            hardware_metrics = self.hardware_manager.get_hardware_metrics()
            
            return {
                'task_id': task_id,
                'success': True,
                'results': {
                    'semantic_analysis': semantic_result,
                    'memory_optimization': memory_result,
                    'cognitive_synthesis': synthesis_result
                },
                'hardware_metrics': hardware_metrics,
                'total_processing_time': sum([
                    semantic_result.get('processing_time', 0),
                    memory_result.get('processing_time', 0),
                    synthesis_result.get('processing_time', 0)
                ])
            }
            
        finally:
            # æ¸…ç†è³‡æº
            self.hardware_manager.release_allocations(task_id)
    
    def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        return {
            'hardware_profile': self.hardware_profile.__dict__,
            'current_metrics': self.hardware_manager.get_hardware_metrics(),
            'acceleration_history': self.acceleration_driver.acceleration_history[-10:],
            'cuda_available': CUDA_AVAILABLE,
            'gpu_count': GPU_COUNT
        }

# æ¸¬è©¦å’Œæ¼”ç¤ºä»£ç¢¼
async def demonstrate_real_retryix():
    """æ¼”ç¤ºçœŸå¯¦RetryIXç³»çµ±"""
    
    print("ğŸš€ å•Ÿå‹•çœŸå¯¦RetryIXç³»çµ±æ¼”ç¤º...")
    
    # åˆå§‹åŒ–å¼•æ“
    engine = RealRetryIXEngine()
    
    # æ¸¬è©¦è¼¸å…¥
    test_inputs = [
        "é€™æ˜¯ä¸€å€‹æ¸¬è©¦èªçŸ¥è™•ç†ç³»çµ±çš„è¼¸å…¥æ–‡æœ¬",
        torch.randn(2, 10, 768),  # éš¨æ©Ÿå¼µé‡
        "Artificial intelligence is transforming our world"
    ]
    
    for i, test_input in enumerate(test_inputs):
        print(f"\nğŸ“Š æ¸¬è©¦ {i+1}: {type(test_input).__name__}")
        
        result = await engine.real_cognitive_processing(
            input_data=test_input,
            processing_mode="comprehensive"
        )
        
        if result.get('success'):
            print(f"âœ… è™•ç†æˆåŠŸï¼Œç¸½æ™‚é–“: {result['total_processing_time']:.3f}ç§’")
            print(f"   GPUåˆ©ç”¨ç‡: {[gpu['load'] for gpu in result['hardware_metrics']['gpu_metrics']]}")
            print(f"   è¨˜æ†¶é«”ä½¿ç”¨: {result['hardware_metrics']['memory_usage']:.1f}%")
        else:
            print(f"âŒ è™•ç†å¤±æ•—: {result.get('error')}")
    
    # ç³»çµ±ç‹€æ…‹å ±å‘Š
    status = engine.get_system_status()
    print(f"\nğŸ“ˆ ç³»çµ±ç‹€æ…‹å ±å‘Š:")
    print(f"   CUDAå¯ç”¨: {status['cuda_available']}")
    print(f"   GPUæ•¸é‡: {status['gpu_count']}")
    print(f"   åŠ é€Ÿæ“ä½œ: {len(status['acceleration_history'])} æ¬¡")

if __name__ == "__main__":
    # å¦‚æœç”¨æˆ¶æä¾›çœŸå¯¦ç¡¬é«”ï¼Œé‹è¡Œæ­¤æ¼”ç¤º
    print("ğŸ”¥ RetryIX çœŸå¯¦ç¡¬é«”å¯¦ç¾å·²æº–å‚™å°±ç·’ï¼")
    print("ğŸ’¡ å¦‚æœä½ æœ‰ç¡¬é«”æ”¯é…èƒ½åŠ›ï¼Œè«‹é‹è¡Œ:")
    print("   python retryix_real_hardware.py")
    print()
    
    # é¡¯ç¤ºç•¶å‰æª¢æ¸¬åˆ°çš„ç¡¬é«”
    hardware = HardwareProfile.detect_current_hardware()
    print("ğŸ–¥ï¸ ç•¶å‰æª¢æ¸¬åˆ°çš„ç¡¬é«”:")
    print(f"   GPU: {len(hardware.gpu_devices)} å€‹")
    print(f"   CPU: {hardware.cpu_cores} æ ¸å¿ƒ")
    print(f"   RAM: {hardware.system_ram} GB")
    
    # è©¢å•æ˜¯å¦é‹è¡Œæ¼”ç¤º
    if input("\næ˜¯å¦é‹è¡ŒçœŸå¯¦ç¡¬é«”æ¼”ç¤ºï¼Ÿ(y/n): ").lower() == 'y':
        asyncio.run(demonstrate_real_retryix())
